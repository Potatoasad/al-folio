<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Foundations of Data Science - Lecture 1 | Asad Hussain</title> <meta name="author" content="Asad Hussain"> <meta name="description" content="Some notes from the first lecture of the course Foundations of Data Science at the Odin Institute at UT Austin. It's great to go into the basics and derive all these fundamental theorems"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://potatoasad.github.io/blog/2023/FODS-lecture-1/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Foundations of Data Science - Lecture 1",
      "description": "Some notes from the first lecture of the course Foundations of Data Science at the Odin Institute at UT Austin. It's great to go into the basics and derive all these fundamental theorems",
      "published": "March 7, 2023",
      "authors": [
        {
          "author": "Asad Hussain",
          "authorURL": "",
          "affiliations": [
            {
              "name": "UT Austin",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Asad </span>Hussain</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Foundations of Data Science - Lecture 1</h1> <p>Some notes from the first lecture of the course Foundations of Data Science at the Odin Institute at UT Austin. It's great to go into the basics and derive all these fundamental theorems</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#snr-of-two-modes">SNR of two modes</a></div> <div><a href="#checking-the-dependence-of-snr-on-amplitude-ratio">Checking the dependence of SNR on amplitude ratio</a></div> </nav> </d-contents> <h1 id="cse-382m-foundational-techniques-in-machine-learning--data-science">CSE 382M: Foundational Techniques in Machine Learning &amp; Data Science</h1> <p><strong>Reference Textbooks</strong>: Foundations of Data Science Avrim Blum, John Hopcroft (online version up there)</p> <h1 id="high-dimensions">High Dimensions</h1> <p>Understanding high-dimensional spaces, such as $\mathbb{R}^d$, is important in various areas of machine learning and data analysis. As $d$ approaches infinity, the nature of the space can become difficult to comprehend.</p> <p>For example, consider images with dimensions $1024 \times 1024$, where the dimensionality $d$ equals the number of pixels in the image.</p> <p>In order to explore and understand high-dimensional spaces, we can use random vectors to sample this space:</p> <p>Let $X$ be a random vector, where the squared norm (magnitude) of the vector $X$ is represented as $|X|^2 = \sum^d_{i} x_i^2$. The norm is essentially the sum of the squares of the random scalar components.</p> <p>Algorithms in machine learning and data analysis are often initialized randomly in $\mathbb{R}^n$, which makes understanding the behavior of random variables in high-dimensional spaces crucial.</p> <h4 id="random-variables">Random Variables:</h4> <p>There are various types of random variables, such as Gaussian, Bernoulli, etc. The Gaussian (normal) distribution is defined by the probability density function:</p> \[\frac{1}{2\pi\sigma} \exp{(-\frac{(x-\mu)^2}{\sigma^2})}\] <p>where $\mu$ is the mean and $\sigma$ is the standard deviation of the distribution.</p> <p>Other common random variables, like the Bernoulli distribution, describe different types of probabilistic behavior and are useful for understanding the properties and implications of high-dimensional spaces.</p> <h2 id="expectationexpected-value">Expectation/Expected Value:</h2> <p>Discrete:</p> \[\mathbb{E}(X) = \sum_i \mathbb{P}(X=x_i) x_i\] <p>Continuous</p> \[\mathbb{E}(X) = \int p(x) x dx\] <p>Gaussian mean value:</p> <p>\(\begin{align} \mathbb{E}(X) &amp;= \int p(x) x dx \\ &amp;= \int \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x-\mu)^2}{2}) x dx \\ &amp;\textrm{ Transform } x' = x - \mu \\ &amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) (x' + \mu) dx' \\ &amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) (x') dx' + \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) (\mu) dx'\\ &amp;\textrm{First term is 0 because x' is odd} \\ &amp;= 0 + \mu \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) dx'\\ &amp;= \mu \\ \implies \mathbb{E}(X) &amp;= \mu \end{align}\) Gaussian Integral: \(\begin{align} I &amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{x^2}{2}) dx \\ I^2 &amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{x^2}{2}) \frac{1}{\sqrt{2\pi}} \exp(-\frac{y^2}{2}) dx dy \\ I^2 &amp;= \frac{1}{2\pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp(-\frac{x^2 + y^2}{2}) dx dy \\ I^2 &amp;= \frac{1}{2\pi} \int_{0}^{2\pi} \int_{0}^{\infty} \exp(-\frac{r^2}{2}) r dr d\theta \\ I^2 &amp;= \frac{2\pi}{2\pi} \int_{0}^{\infty} \exp(-\frac{r^2}{2}) r dr d\theta \\ I^2 &amp;= \frac{2\pi}{2\pi} \int_{0}^{\infty} \exp(-\frac{r^2}{2}) r dr \\ I^2 &amp;= \int_{0}^{\infty} \exp(-u) du \\ I^2 &amp;= 1 \\ \implies I &amp;= 1 \end{align}\)</p> <h3 id="properties-of-expectations">Properties of Expectations</h3> \[\begin{align} \mathbb{E}(cX) &amp;= c\mathbb{E}(X) \\ \mathbb{E}(X + Y) &amp;= \mathbb{E}(X) + \mathbb{E}(Y) \\ \mathbb{E}(X \times Y) &amp;= \mathbb{E}(X)\mathbb{E}(Y) \textrm{ only if X and Y are independent} \\ \mathbb{E}(f(X)) &amp;= \int f(x)p(x)dx \\ \end{align}\] <blockquote> <p><strong>Note</strong>: You can take any random variable $X$ , and transform it using any deterministic function $f$, and get another random variable. $Y = f(X) $. This makes $Y$ also a random variable.</p> <p>In addition $f(X)$ and $g(Y)$ are independent if $X$ and $Y$ are independent.</p> </blockquote> <h3 id="variance-of-random-variable">Variance of random variable</h3> \[\begin{align} \textrm{Var}(X) &amp;= \mathbb{E}((X - \mathbb{E}(X))^2) \\ &amp;= \mathbb{E}(X^2) - \mathbb{E}(X)^2 \end{align}\] \[\begin{align} \textrm{Var}(cX) &amp;= c^2 \textrm{Var}(X) \\ \textrm{Var}(X + Y) &amp;= \textrm{Var}(X) + \textrm{Var}(Y) \textrm{ only if X and Y are independent} \\ \end{align}\] <blockquote> <p><strong>Question:</strong> Suppose random variables $x_1, x_2 … x_n$ are independently identically distributed with variance $\sigma^2$. Let us take the average random variable: \(X_{avg} = \frac{1}{n}\sum^n_i X_i\) What is the variance of $X_{avg}$ ?</p> <p><strong>Answer:</strong></p> \[\begin{align*} Var(X_{avg}) = Var(\frac{1}{n}\sum_i^n X_i) \\ Var(X_{avg}) = \frac{1}{n^2}\sum_i^n Var(X_i) \\ Var(X_{avg}) = \frac{1}{n^2} n\sigma^2 \\ Var(X_{avg}) = \frac{\sigma^2}{n} \end{align*}\] </blockquote> <p>Example:</p> <p>$X$ Is a discrete RV modelling a fair die. \(\begin{align} \mathbb{E}(X) &amp;= \frac{1}{6}\times 1 + \frac{1}{6}\times 2 + \frac{1}{6}\times 3 + \frac{1}{6}\times 4 + \frac{1}{6}\times 5 + \frac{1}{6}\times 6 = \frac{21}{6} = 3.5 \end{align}\)</p> \[\begin{align} Var(X) &amp;= \frac{1}{6}\times (1-3.5)^2 + \frac{1}{6}\times (2-3.5)^2 + \frac{1}{6}\times (3-3.5)^2 + \frac{1}{6}\times (4-3.5)^2 + \frac{1}{6}\times (5-3.5)^2 + \frac{1}{6}\times (6-3.5)^2 \\ &amp;\approx 2.917 \end{align}\] <h2 id="concentration-inequalities-apparently-very-important">Concentration Inequalities (apparently very important)</h2> <h3 id="the-general-form">The General Form</h3> <p>We have random RVs $X_1, X_2 … X_n$ (can be correlated) and we want to garuntee that $\bar{X} = \frac{\sum_i^n X_i}{n}$ is close to it’s mean/expectation with high probability i.e.: \(\mathbb{P}\bigg[\frac{\sum_i^n X_i}{n} - \mathbb{E}\bigg(\frac{\sum_i^n X_i}{n}\bigg) \geq \epsilon \bigg] \textrm{ gets smaller and smaller with n}\)</p> <h3 id="markovs-inequality">Markov’s Inequality</h3> <p>Take a non-negative scalar values RV $X$. (i.e. $X \geq 0$ always) Then for every positive cutoff $a$, we have the following: \(\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a}\) The right tail probability is smaller than the expectation value divided by the tail begining</p> <p>Another form of this: \(\mu = \mathbb{E}(X) \\ \mathbb{P}(X \geq k\mu) \leq \frac{1}{k}\)</p> <blockquote> <p><strong>The probability of being atleast within $k$ means of $0$ is at most $1/k$</strong></p> </blockquote> <h4 id="proof-of-markovs-inequality">Proof of Markov’s Inequality</h4> \[\begin{align} \mathbb{E}(X) &amp;= \int_0^a p(x) x dx + \int_a^\infty p(x)x dx \\ \textrm{ second integrand's x is bigger than $a$} \\ \mathbb{E}(X) &amp;\geq \int_0^a p(x) x dx + \int_a^\infty p(x)a dx \\ \textrm{ first integrand has to be bigger than $0$} \\ \mathbb{E}(X) &amp;\geq a\int_a^\infty p(x) dx \\ \mathbb{E}(X) &amp;\geq a\mathbb{P}(X \geq a) \\ \mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a} \end{align}\] <h3 id="inequality-is-weak">Inequality is weak</h3> <p>You can try this out for the dice and find that’s a pretty weak constraint.</p> <h2 id="lecture-2">Lecture 2</h2> <p>If we have bounds on “higher moments” of $X$, then we can do better than Markov’s inequality.</p> <p>For every positive even integer $r$, even if $X$ takes negative values.</p> <p>Then \(P(X \geq a) \leq P(X^r \leq a^r) \textrm{ this is an equality if } X \geq 0\) This implies that: \(P(X \geq a) \leq \frac{\mathbb{E}(X^r)}{a^r}\) Only if $\mathbb{E}(X^r)$ is finite (if its infinite then it doesn’t really say an)</p> <p>The first application of this is chebyshev’s inequality.</p> <h3 id="chebyshevs-inequality">Chebyshev’s Inequality</h3> \[P(|X - \mathbb{E}(X)| \geq c) \leq \frac{Var(X)}{c^2}\] <p>Equivalently $\mathbb{E}(X) = \mu$, $Var(X) = \sigma^2$ and $c = k\sigma$ then the inequality reads: \(P(\|X-\mu\| \geq k\sigma) \leq \frac{1}{k^2}\)</p> <blockquote> <p><strong>Any RV is more than $k$ standard deviations away from its mean $1/k^2$ of the time</strong></p> <p>e.g: Any variable (with finite variance) is 2 std away from it’s mean less than 1/4th of the time</p> </blockquote> <h5 id="proof">Proof:</h5> <p>Use Markov’s inequality \(P((X-\mathbb{E}(X))^2 \geq c^2) \leq \frac{(X-\mathbb{E}(X))^2}{c^2} \\ P(\|X - \mathbb{E}(X)\| \geq c) \leq P((X-\mathbb{E}(X))^2 \geq c^2) \leq \frac{Var(X)}{c^2} \\\)</p> <h3 id="master-tail-bound">Master Tail Bound</h3> <p>Let $Y = X_1 + X_2 + … + X_n$ where the $X_i$ are mutually independent <strong>but not necessarilly identically distributed</strong> with conditions:</p> <ol> <li>Centered: $\mathbb{E}(X_i) = 0$</li> <li>Variance Bound: $\mathbb{E}(X^2_i) \leq \sigma^2$</li> <li>Higher moment bound: $|\mathbb{E}(X^s_i)| \leq s!\sigma^2$ for upto $s=3,4,…,\lfloor\frac{a^2}{4n\sigma^2} \rfloor$</li> </ol> <p>If $a$ is <strong>not too big:</strong> $0 &lt; a \leq \sqrt{2}n \sigma^2$ Then this variable’s upper bound <strong>decays like a gaussian</strong>: \(P(\|Y\| \geq a) \leq 3e^{-\frac{a^2}{12n\sigma^2}}\)</p> <h5 id="comments">Comments</h5> <p>Can rewrite the conclusion in another way ($a = \sqrt{n} t$): \(P(\frac{\|Y\|}{\sqrt{n}} \geq t) \leq 3 e^{-\frac{t^2}{12\sigma^2}}\) Can also rewrite using a sample average $a = nb$: \(P(\frac{\|Y\|}{n} \geq b) \leq 3 e^{-\frac{nb^2}{12\sigma^2}}\) Comparison to the <strong>Central Limit Theorum (CLT)</strong>:</p> <p>Let $X_1, X_2, …X_n$ be iid with mean 0 and variance $\sigma^2$ each. \(\lim_{n \to \infty}\frac{Y}{\sqrt{n}} \sim \mathcal{N}(0,\sigma^2)\)</p> \[P(\frac{\|Y\|}{\sqrt{n}} \geq t) \leq e^{-\frac{t^2}{2\sigma^2}} \textrm{ by CLT} \\\] \[P(\frac{\|Y\|}{\sqrt{n}} \geq t) \leq 3e^{-\frac{t^2}{12\sigma^2}} \textrm{ by Master Tail Bound} \\\] <p>Takeaway: <strong>Master Tail bound is a non-asymptotic, slightly weaker CLT-type bound</strong></p> <h3 id="proof-of-master-tail-bound">Proof of Master Tail Bound</h3> \[P(\|Y\| \geq a) = P(Y^r \geq a^r) \textrm{ for any positive integer r} \\ P(\|Y\| \geq a) \leq \frac{\mathbb{E}(X^r)}{a^r}\] <p>Use the multinomial expansion: \(X^r = (X_1 + X_2 + .. X_n)^r = \sum_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}X_1^{r_1} X_2^{r_2} ... X_n^{r_n}\)</p> \[\mathbb{E}(X^r) = \mathbb{E}((X_1 + X_2 + .. X_n)^r) = \sum_{r_1+r_2 + ...r_n = r} \frac{r!}{r_1!r_2!...r_n!}\mathbb{E}(X_1^{r_1} X_2^{r_2} ... X_n^{r_n})\] <p>Use mutual independence: \(\mathbb{E}(X^r) = \sum_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}\mathbb{E}(X_1^{r_1}) \mathbb{E}(X_2^{r_2}) ... \mathbb{E}(X_n^{r_n})\)</p> <blockquote> <p>Note that if there is any term in the expansion with $r_i = 1$, then by $\mathbb{E}(X_i) = 0$, that whole term is $0$</p> </blockquote> \[\mathbb{E}(X^r) = \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}\mathbb{E}(X_1^{r_1}) \mathbb{E}(X_2^{r_2}) ... \mathbb{E}(X_n^{r_n})\] <p>Use the triangle inequality: \(\mathbb{E}(X^r) \leq |\mathbb{E}(X^r)| \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}|\mathbb{E}(X_1^{r_1}) \mathbb{E}(X_2^{r_2}) ... \mathbb{E}(X_n^{r_n})|\)</p> \[\mathbb{E}(X^r) \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}|\mathbb{E}(X_1^{r_1})|| \mathbb{E}(X_2^{r_2}) |... |\mathbb{E}(X_n^{r_n})|\] \[\begin{align} \mathbb{E}(X^r) \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!} (\sigma^2)^{\textrm{\# of non zero } r_i \textrm{s}} r_1!r_2!..r_n! \\ \mathbb{E}(X^r) \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r} (\sigma^2)^{\textrm{\# of non zero } \textrm{s}} r!\\ \textrm{ SOME COMBINATORIAL TRICKERY LOOK AT THE BOOK} \\ \mathbb{E}(X^r) \leq \frac{r!}{(r/2)!}2^{r/2}(n\sigma^2)^{r/2} \end{align}\] <p>Hence we have: \(P(\|Y\|\geq a) \leq \frac{\mathbb{E}(X^r)}{a^r} \leq \frac{r!}{a^r(r/2)!}2^{r/2}(n\sigma^2)^{r/2} = g(r)\) Let’s find the $r$ that minimizes the right hand side, and gives us the best bound: \(\frac{r!}{a^r(r/2)!} \textrm{ is at most } r^{r/2}\) and hence: \(g(r) \leq \frac{r!}{a^r(r/2)!}2^{r/2}(n\sigma^2)^{r/2} \leq \bigg(\frac{2rn\sigma^2}{a^2}\bigg)^{r/2}\) We then choose $r$ to be the largest even number s.t $r \leq \frac{a^2}{6n\sigma^2}$</p> <p>This makes it such that: \(g(r) \leq (\frac{1}{3})^{r/2}\) this means that for the lols we can do $3 \to e$: \(g(r) \leq (\frac{1}{e})^{r/2} = e^{-r/2} \leq e^{-\frac{a^2}{12n\sigma^2} + \frac{2}{2}}\) Again because we choose the smallest even number do: \(g(r) \leq e^{-\frac{a^2}{12n\sigma^2} + 1} \leq 3 e^{-\frac{a^2}{12n\sigma^2}}\) This gives us our bound: \(P(\|Y\|\geq a) \leq 3 e^{-\frac{a^2}{12n\sigma^2}}\)</p> <h3 id="applying-the-tail-bound">Applying the tail bound</h3> <p>Assume $X_i \sim Bernoulli(p)$, then what is the tail bound on $Y = \sum^n_i X_i$, where now $Y \sim Binomial(n,p)$ ?</p> <p>For any $c \in [0,1]$ whe have: \(P(\|\frac{Y}{n} - p\| \geq \sqrt2 c p (1-p)) \leq 3 e^{-\frac{np(1-p)c^2}{6}}\) Proof:</p> <p>Let $Y_i = X_i - p$ then $\mathbb{E}(Y_i) = 0$ and $Var(Y_i) = Var(X_i) = \mathbb{E}(Y_i^2)= p(1-p) $ \(\mathbb{E}(Y_i^s) = \|p(1-p)^s + (1-p)(-p)^s\| \leq p(1-p)^s + (1-p)(-p)^s \\ \mathbb{E}(Y_i^s) \leq p(1-p)^2 + (1-p)(p)^2 \\ \mathbb{E}(Y_i^s) \leq \frac{p(1-p)}{3} \\\) Plug in $\sigma^2 = p(1-p)$ into MTB and get the bound as given before.</p> <h1 id="lecture-3">Lecture 3</h1> <p>Chernov bound discussion:</p> <p>Assume $X_i \sim Bernoulli(p)$, then what is the tail bound on $Y = \sum^n_i X_i$, where now $Y \sim Binomial(n,p)$ ?</p> <p>For any $c \in [0,1]$ whe have: \(P(\|\frac{Y}{n} - p\| \geq \sqrt2 c p (1-p)) \leq 3 e^{-\frac{np(1-p)c^2}{6}}\)</p> <blockquote> <p><strong>remark</strong>: How sharp is this estimate?</p> <p><strong>Ans</strong>: Not so tight. One can fix $n,p $ and $c$, and just estimate this probabiliity.</p> </blockquote> <p>The constraint is not too tight. See below:</p> <p><img src="2023-03-25-FODS-lecture-1.assets/chernovbound.svg" alt="chernovbound"></p> <p>Other ineqs: Bernstein’s inequality and Hoeffding’s inequality. But these follow approximately the same methods we used i.e. Applying markov to a modification of a Random RV.</p> <h2 id="curses-and-blessings-in-high-dimensions">Curses and Blessings in High dimensions</h2> <h2 id="curses">Curses:</h2> <p>$\mathbb{R}^d$ where $d$ is large, e.g. $10^6$</p> <blockquote> <p><strong>Function approximation:</strong></p> <p>Given points $(x_1,y_1),(x_2,y_2),…, (x_n,y_n)$ in $\mathbb{R}^d \times\mathbb{R}^d$, find a good $f: \mathbb{R}^d \to \mathbb{R}^d$ such that $f(x_i) = y_i$ or $f(x_i) \approx y_i$</p> <p>Approach: Linear Interpolation, polynomial interpolation, spline interpolation. These are all piecewise interpolations, which can be a problem for large $d$</p> <p><strong>Question:</strong> How many d-dim cubes with side length $\epsilon$ are needed to fill a cube with side length 1</p> <p><strong>Answer</strong>: $(\frac{1}{\epsilon})^d$</p> </blockquote> <p>We need a way to overcome large $d$, i.e. $d$ is exponentional. Because piecewise clearly will not be doable.</p> <p>We overcome this by putting assumptions on $f$. Examples are:</p> <ul> <li>$f$ is affine linear and the matrix that represents that linear function is low-rank</li> <li>$f$ is …</li> </ul> <h2 id="blessings">Blessings:</h2> <p>Various natural probability distributions on $\mathbb{R}^d$ will concentrate more and more as $d$ increases.</p> <p>But first we need to talk about spheres and balls in $\mathbb{R}^d$. Main theorum is that a Gaussian annulus theorum.</p> <h4 id="high-dimensional-ball">High Dimensional Ball</h4> <p>Definition: A $d$ dimensional ball $B^d$ is given \(B^d = \{x \in \mathbb{R}^d : x_1^2 + x_2^2 + ... + x_d^2 \leq 1\}\)</p> <h4 id="rejection-sampling">Rejection Sampling</h4> <p>One way is rejection sampling which is BAD for high dimensions. \(vol(B^d) = \frac{2\pi}{\Gamma(\frac{d}{2})d}\) <strong>Corollary 1</strong>: \(vol(B^d) \to 0 \textrm{ as } d \to 0\) <strong>Corollary 2</strong>:</p> <p>The acceptance probability of the rejection sampling scheme goes to $0$ as $d \to \infty$</p> <h4 id="solution">Solution</h4> <p>Sample from a spherical gaussian in $\mathbb{R}^d$.</p> \[X \sim \mathcal{N}(0,\mathbb{I}) \\ p = \prod_i \phi(x_i)dx_i\] <p>Note that this distribution is rotationally symmetric, since it depends on a rotationally invariant scalar $X^T X$.</p> <p>Note then that:</p> \[Y = \frac{X}{\|X\|_2} \textrm{ is uniform on the sphere}\] <p>Then let there be a positive scalar whose density function is:</p> \[p(\rho) \propto \rho^{d-1}\] <p>This must be because in $n$ dimensional polar coordinates:</p> \[dx_1\wedge dx_2 \wedge ... \wedge dx_n = \rho^{d-1}d\rho \wedge d^{d-1}\Omega\] <p>So we have the fact that you can get the uniform distribution by:</p> <ul> <li>Define $Y = \frac{X}{|X|_2}$</li> <li>Then get a random variable $\rho \in [0,1]$ with density $\rho^{d-1}$</li> <li>$U \sim \frac{\rho X}{|X|_2}$ is uniform in the unit ball!</li> </ul> <h3 id="gaussian-annulus-theorem">Gaussian Annulus Theorem</h3> <p>Let $X \sim \mathcal{N}(0,I)$ For any $\beta \in [0,\sqrt{d}]$: \(P(|\ \|X\|_2 - \sqrt{d}\ | \geq \beta) \leq 3e^{-\frac{\beta^2}{96}}\) We can prove this using the master tail bound. Let there be $d$ independent random variables $x_i^2 - 1$ which are distributed as $\chi^2$ distributions. Then we define \(Y = \sum^d_i (x_i^2-1)\) Hence the bound above just becomes: \(P(|\ \|X\|_2 - \sqrt{d}\ | \geq \beta) = P(|\ \|X\|_2^2 - d\ | \geq \beta^2) = P(\|Y\| \geq \beta^2)\) Now we must have that: \(\mathbb{E}(x^2_i - 1) = 0 \\ \mathbb{E}((x^2_i - 1)^2) \leq \sigma^2 \\ \|\mathbb{E}((x^2_i - 1)^s)\| \leq \sigma^2 s! \\ \implies P(\|Y\| \geq \beta^2) \leq 3e^{-\frac{\beta^4}{12d\sigma^2}}\) To show the above we can do the following bounding process: \(\|\mathbb{E}((x^2_i - 1)^s)\| \leq \mathbb{E}(\|(x^2_i - 1)^s\|) \leq \mathbb{E}(1 + x^{2s}_i) = 1 + \mathbb{E}(x^{2s}_i) \leq 2^s s!\) Apply master tail bound to $\frac{Y_i}{2}$</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Asad Hussain. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>