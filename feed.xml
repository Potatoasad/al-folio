<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://potatoasad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://potatoasad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-05T18:15:49+00:00</updated><id>https://potatoasad.github.io/feed.xml</id><title type="html">blank</title><subtitle>I&apos;m a PhD Student in the Physics department at UT Austin (Weinberg Theory Group). I work on creating precision tests of theories beyond Einstein&apos;s theory of General Relativity, by looking at specific frequencies in the gravitational waves produced by black hole mergers. I also play with data, write code to generate music and whatever else is fun.</subtitle><entry><title type="html">Astrophysical Amplitude Priors</title><link href="https://potatoasad.github.io/blog/2023/astrophysical-amplitude-priors/" rel="alternate" type="text/html" title="Astrophysical Amplitude Priors"/><published>2023-03-07T00:00:00+00:00</published><updated>2023-03-07T00:00:00+00:00</updated><id>https://potatoasad.github.io/blog/2023/astrophysical-amplitude-priors</id><content type="html" xml:base="https://potatoasad.github.io/blog/2023/astrophysical-amplitude-priors/"><![CDATA[<h1 id="astrophysical-prior-analysis">Astrophysical Prior Analysis</h1> <p>Let’s try and set up the distribution for the prior on the ringdown amplitude at the detector. We will do so by considering the distribution of events in space and put some assumptions on the distribution on the source amplitude.</p> <h3 id="event-position-distribution">Event position distribution</h3> <p>What we really want is that we have space with a uniform scattering of black hole collisions. We will consider the position to be uniform inside a 3-D ball $\mathcal{B}_L$.</p> \[\vec{\begin{bmatrix}\textrm{Position of}\\\textrm{given event}\end{bmatrix}} = \vec{x} = \lim_{L\to\infty} \vec{U}_{\mathcal{B}_{L}}\] <p>In polar coordinates this transforms as:</p> \[\mathcal{P} = dx\wedge dy\wedge dz = r^2 \sin \theta dr\wedge d\theta \wedge d\phi = d(\frac{r^3}{3})\wedge d^2\Omega\] <p>Which gives us that the $r^3$ random variable is uniform. Hence:</p> \[\vec{\begin{bmatrix}\textrm{Position of}\\\textrm{given event}\end{bmatrix}} = [r^3 \sim U_{[0,L^3]}, \theta \sim U_{[-\pi,\pi]}, \phi \sim U_{[0,2\pi]}]\] <h3 id="event-amplitude-distribution-at-the-source">Event amplitude distribution at the source</h3> <p>Let us assume that each event choses it’s amplitude completely uniformly. Each collision can have ANY amplitude from 0 to some big number like $A_{\max}$.</p> \[\begin{bmatrix}\textrm{Amplitude of a given}\\\textrm{event at the source}\end{bmatrix} = A_0 \sim U_{[0,A_{\max}]}\] <p>Now ofcourse, we can make this amplitude depend on the masses, spins and other variables intrinsic of the merger event. However, since we’re constructing a prior, we will just leave it as independent and uninformative.</p> <h3 id="amplitude-at-detector">Amplitude at Detector</h3> <p>Using the amplitude distribution of the events at source, and the event position distribution we can find the Amplitude distribution at the detector. We know that the amplitude of gravitational wave event falls off as $1/r$. Hence:</p> \[A_{\textrm{det}} = \frac{A_{0}}{r} = \frac{A_{0}}{|\vec{x}|}\] \[A_{det}^2 \propto \frac{A_{0}^2}{r^2}\\ \textrm{with }r^3 \sim U_{[0,L^3]} \textrm{ and } A_0 \sim U_{[0,A_{max}]}\] <p>Now we can compute the result here and find that:</p> \[P(A^{\textrm{det}} = a) = \frac{3}{4a_0} \begin{cases} 1 &amp; a &lt; a_0 \\ (\frac{a}{a_0})^{-4} &amp; a \geq a_0 \end{cases} \\ \textrm{where } a_0 = \frac{A_{\max}}{L}\] <p>Which we can plot for specific cases. For $A_{\max} = 10,000$ and $L = 10,000$ (measured in units of some reference distance where we detect $A_0$ for each event) we get:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/SNR-distribution-5121189.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/SNR-distribution-5121189.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/SNR-distribution-5121189.svg-1400.webp"/> <img src="/assets/img/SNR-distribution-5121189.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In most astrophysical cases, the amplitudes are small enough, and the universe large enough that we are in the regime where $p(\rho) \propto \rho^{-4}$</p> <h3 id="event-amplitude-ratio">Event amplitude ratio</h3> <p>We can allow the amplitude ratio to be any number between 0 and 2. This amplitude ratio can and does change the calculation of the SNR. We will set this ratio to be uniform for now:</p> \[\alpha \sim U_{[0,2]}\] <p>The SNR depends on the detector amplitude as: \(\begin{align} \rho^2 &amp;= \langle h_0| h_0 \rangle + 2 \alpha \langle h_0| h_1 \rangle + \alpha^2 \langle h_1| h_1 \rangle \end{align}\)</p> \[\rho^2 \propto A_{\textrm{det}}^2(C_0 + C_1\alpha + C_2 \alpha^2)\] <p>In general the constants above depend on the polarization parameters and frequencies of the two modes, as well as the covariance matrix of the noise used to do the inner product:</p> \[C_i = C_i\bigg(\begin{bmatrix}\epsilon_1 \\ \epsilon_2\end{bmatrix}, \begin{bmatrix}\theta_1 \\ \theta_2\end{bmatrix},\begin{bmatrix}\phi_1 \\ \phi_2\end{bmatrix}, \begin{bmatrix}\omega_1 \\ \omega_2\end{bmatrix}, \begin{bmatrix}\tau_1 \\ \tau_2\end{bmatrix}, \Sigma\bigg)\]]]></content><author><name>Asad Hussain</name></author><summary type="html"><![CDATA[Coming up with priors for how many gravitational waves we should expect to see at of a given amplitude]]></summary></entry><entry><title type="html">SNR Mode Contributions</title><link href="https://potatoasad.github.io/blog/2023/snr-mode-contributions/" rel="alternate" type="text/html" title="SNR Mode Contributions"/><published>2023-03-07T00:00:00+00:00</published><updated>2023-03-07T00:00:00+00:00</updated><id>https://potatoasad.github.io/blog/2023/snr-mode-contributions</id><content type="html" xml:base="https://potatoasad.github.io/blog/2023/snr-mode-contributions/"><![CDATA[<h2 id="snr-of-two-modes">SNR of two modes:</h2> <p>Consider two damped sinosoidal modes (a fundamental and an overtone) with an amplitude ratio $\alpha$ between the two modes. We set the two modes as</p> \[h(t) = h_0(t) + \alpha h_1(t)\] <p>We can calculate the SNR under the assumptions that the noise is white, and hence for a quick and dirty approximation, use the normal $L^2$ inner product over the function space to be a measure of the squared SNR. We then have:</p> \[\begin{align} \rho^2 &amp;= \langle h_0| h_0 \rangle + 2 \alpha \langle h_0| h_1 \rangle + \alpha^2 \langle h_1| h_1 \rangle \end{align}\] <p>Assuming that</p> \[h(t | \omega, \gamma, \phi) = e^{-\gamma t} \cos(\omega t - \phi) \\\] <p>or alternatively in terms of the quality factor:</p> \[h(t|Q\gamma, \gamma, \phi) = e^{-\gamma t}\cos(Q\gamma t - \phi)\] \[h(t) = h(t | Q_0\gamma_0, \gamma_0, \phi_0) + \alpha h(t | Q_1\gamma_1, \gamma_1, \phi_1)\] <p>Plugging it in and using mathematica we get that the form of the squared SNR is, ofcourse quadratic in $\alpha$.</p> \[\rho^2 = C_0 + \alpha C_1 + \alpha^2 C_2\] <p>Let</p> \[A = \frac{\gamma_0 Q_0 - \gamma_1 Q_1}{\gamma_0 + \gamma_1} \\ B = \frac{\gamma_0 Q_0 + \gamma_1 Q_1}{\gamma_0 + \gamma_1} \\\] <p>Then:</p> \[C_0 = \frac{1}{4\gamma_0} \frac{1 + \cos(2\phi_0) + Q_0\sin(2\phi_0) + Q_0^2}{1 + Q_0^2}\] \[C_1 = \frac{1}{\gamma_0 + \gamma_1} \bigg(\frac{\cos(\phi_0 - \phi_1) + A \sin(\phi_0 - \phi_1)}{1 + A^2} + \frac{\cos(\phi_0 + \phi_1) + B \sin(\phi_0 + \phi_1)}{1 + B^2} \bigg)\] \[C_2 = \frac{1}{4\gamma_1}\bigg( \frac{1 + \cos \left(2 \phi _1\right) + Q_1 \sin \left(2 \phi _1\right) + Q_1^2}{1+ Q_1^2} \bigg)\] <h3 id="checking-the-dependence-of-snr-on-amplitude-ratio">Checking the dependence of SNR on amplitude ratio</h3> <p>We checked the dependence of the SNR on the ampitude ratio for some specifically chosen signal parameters (but not any edge case ones), to see how it scales.</p> <p>We set the first mode’s amplitude $A_0 = 10^{-20}$ and see the dependence of the SNR on $\alpha$. The noise that was injected was a particular realisation of the synthetic noise from GW150914. The rest of the parameters were fixed as below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">model</span><span class="sh">'</span> <span class="p">:</span> <span class="sh">'</span><span class="s">mchi</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">a</span><span class="sh">'</span> <span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">A0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">*</span><span class="n">A0</span><span class="p">]),</span>
        <span class="sh">'</span><span class="s">M</span><span class="sh">'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">chi</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">modes</span><span class="sh">'</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)],</span>
        <span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">ellip</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">phi</span><span class="sh">'</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> 
        <span class="sh">'</span><span class="s">ra</span><span class="sh">'</span> <span class="p">:</span> <span class="mf">1.95</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">dec</span><span class="sh">'</span><span class="p">:</span> <span class="o">-</span><span class="mf">1.27</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">psi</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.82</span>
    <span class="p">}</span>
</code></pre></div></div> <p>Then we found the dependence of the SNR defined above and the amplitude ratio between the two modes given by $\alpha$.</p> <p>We get the following plot for the SNR squared vs alpha:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/SNRsquaredPlot-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/SNRsquaredPlot-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/SNRsquaredPlot-1400.webp"/> <img src="/assets/img/SNRsquaredPlot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When we try to fit the above to the equation $\rho^2 \propto C_0 + C_1\alpha + C_2\alpha^2$, then we get the fit:</p> \[\rho^2 = 350.4 x^2 + 890.9 x + 1132\] <p>Which gives the above graph.</p>]]></content><author><name>Asad Hussain</name></author><category term="gravitationalwaves"/><category term="blackholes"/><category term="stochasticprocess"/><category term="noise"/><summary type="html"><![CDATA[What effect do the modes in a particular]]></summary></entry><entry><title type="html">Foundations of Data Science - Lecture 1</title><link href="https://potatoasad.github.io/blog/2023/FODS-lecture-1/" rel="alternate" type="text/html" title="Foundations of Data Science - Lecture 1"/><published>2023-03-07T00:00:00+00:00</published><updated>2023-03-07T00:00:00+00:00</updated><id>https://potatoasad.github.io/blog/2023/FODS-lecture-1</id><content type="html" xml:base="https://potatoasad.github.io/blog/2023/FODS-lecture-1/"><![CDATA[<h1 id="cse-382m-foundational-techniques-in-machine-learning--data-science">CSE 382M: Foundational Techniques in Machine Learning &amp; Data Science</h1> <p><strong>Reference Textbooks</strong>: Foundations of Data Science Avrim Blum, John Hopcroft (online version up there)</p> <h1 id="high-dimensions">High Dimensions</h1> <p>Understanding high-dimensional spaces, such as $\mathbb{R}^d$, is important in various areas of machine learning and data analysis. As $d$ approaches infinity, the nature of the space can become difficult to comprehend.</p> <p>For example, consider images with dimensions $1024 \times 1024$, where the dimensionality $d$ equals the number of pixels in the image.</p> <p>In order to explore and understand high-dimensional spaces, we can use random vectors to sample this space:</p> <p>Let $X$ be a random vector, where the squared norm (magnitude) of the vector $X$ is represented as $|X|^2 = \sum^d_{i} x_i^2$. The norm is essentially the sum of the squares of the random scalar components.</p> <p>Algorithms in machine learning and data analysis are often initialized randomly in $\mathbb{R}^n$, which makes understanding the behavior of random variables in high-dimensional spaces crucial.</p> <h4 id="random-variables">Random Variables:</h4> <p>There are various types of random variables, such as Gaussian, Bernoulli, etc. The Gaussian (normal) distribution is defined by the probability density function:</p> \[\frac{1}{2\pi\sigma} \exp{(-\frac{(x-\mu)^2}{\sigma^2})}\] <p>where $\mu$ is the mean and $\sigma$ is the standard deviation of the distribution.</p> <p>Other common random variables, like the Bernoulli distribution, describe different types of probabilistic behavior and are useful for understanding the properties and implications of high-dimensional spaces.</p> <h2 id="expectationexpected-value">Expectation/Expected Value:</h2> <p>Discrete:</p> \[\mathbb{E}(X) = \sum_i \mathbb{P}(X=x_i) x_i\] <p>Continuous</p> \[\mathbb{E}(X) = \int p(x) x dx\] <p>Gaussian mean value:</p> <p>\(\begin{align} \mathbb{E}(X) &amp;= \int p(x) x dx \\ &amp;= \int \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x-\mu)^2}{2}) x dx \\ &amp;\textrm{ Transform } x' = x - \mu \\ &amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) (x' + \mu) dx' \\ &amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) (x') dx' + \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) (\mu) dx'\\ &amp;\textrm{First term is 0 because x' is odd} \\ &amp;= 0 + \mu \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{(x')^2}{2}) dx'\\ &amp;= \mu \\ \implies \mathbb{E}(X) &amp;= \mu \end{align}\) Gaussian Integral: \(\begin{align} I &amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{x^2}{2}) dx \\ I^2 &amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \exp(-\frac{x^2}{2}) \frac{1}{\sqrt{2\pi}} \exp(-\frac{y^2}{2}) dx dy \\ I^2 &amp;= \frac{1}{2\pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp(-\frac{x^2 + y^2}{2}) dx dy \\ I^2 &amp;= \frac{1}{2\pi} \int_{0}^{2\pi} \int_{0}^{\infty} \exp(-\frac{r^2}{2}) r dr d\theta \\ I^2 &amp;= \frac{2\pi}{2\pi} \int_{0}^{\infty} \exp(-\frac{r^2}{2}) r dr d\theta \\ I^2 &amp;= \frac{2\pi}{2\pi} \int_{0}^{\infty} \exp(-\frac{r^2}{2}) r dr \\ I^2 &amp;= \int_{0}^{\infty} \exp(-u) du \\ I^2 &amp;= 1 \\ \implies I &amp;= 1 \end{align}\)</p> <h3 id="properties-of-expectations">Properties of Expectations</h3> \[\begin{align} \mathbb{E}(cX) &amp;= c\mathbb{E}(X) \\ \mathbb{E}(X + Y) &amp;= \mathbb{E}(X) + \mathbb{E}(Y) \\ \mathbb{E}(X \times Y) &amp;= \mathbb{E}(X)\mathbb{E}(Y) \textrm{ only if X and Y are independent} \\ \mathbb{E}(f(X)) &amp;= \int f(x)p(x)dx \\ \end{align}\] <blockquote> <p><strong>Note</strong>: You can take any random variable $X$ , and transform it using any deterministic function $f$, and get another random variable. $Y = f(X) $. This makes $Y$ also a random variable.</p> <p>In addition $f(X)$ and $g(Y)$ are independent if $X$ and $Y$ are independent.</p> </blockquote> <h3 id="variance-of-random-variable">Variance of random variable</h3> \[\begin{align} \textrm{Var}(X) &amp;= \mathbb{E}((X - \mathbb{E}(X))^2) \\ &amp;= \mathbb{E}(X^2) - \mathbb{E}(X)^2 \end{align}\] \[\begin{align} \textrm{Var}(cX) &amp;= c^2 \textrm{Var}(X) \\ \textrm{Var}(X + Y) &amp;= \textrm{Var}(X) + \textrm{Var}(Y) \textrm{ only if X and Y are independent} \\ \end{align}\] <blockquote> <p><strong>Question:</strong> Suppose random variables $x_1, x_2 … x_n$ are independently identically distributed with variance $\sigma^2$. Let us take the average random variable: \(X_{avg} = \frac{1}{n}\sum^n_i X_i\) What is the variance of $X_{avg}$ ?</p> <p><strong>Answer:</strong></p> \[\begin{align*} Var(X_{avg}) = Var(\frac{1}{n}\sum_i^n X_i) \\ Var(X_{avg}) = \frac{1}{n^2}\sum_i^n Var(X_i) \\ Var(X_{avg}) = \frac{1}{n^2} n\sigma^2 \\ Var(X_{avg}) = \frac{\sigma^2}{n} \end{align*}\] </blockquote> <p>Example:</p> <p>$X$ Is a discrete RV modelling a fair die. \(\begin{align} \mathbb{E}(X) &amp;= \frac{1}{6}\times 1 + \frac{1}{6}\times 2 + \frac{1}{6}\times 3 + \frac{1}{6}\times 4 + \frac{1}{6}\times 5 + \frac{1}{6}\times 6 = \frac{21}{6} = 3.5 \end{align}\)</p> \[\begin{align} Var(X) &amp;= \frac{1}{6}\times (1-3.5)^2 + \frac{1}{6}\times (2-3.5)^2 + \frac{1}{6}\times (3-3.5)^2 + \frac{1}{6}\times (4-3.5)^2 + \frac{1}{6}\times (5-3.5)^2 + \frac{1}{6}\times (6-3.5)^2 \\ &amp;\approx 2.917 \end{align}\] <h2 id="concentration-inequalities-apparently-very-important">Concentration Inequalities (apparently very important)</h2> <h3 id="the-general-form">The General Form</h3> <p>We have random RVs $X_1, X_2 … X_n$ (can be correlated) and we want to garuntee that $\bar{X} = \frac{\sum_i^n X_i}{n}$ is close to it’s mean/expectation with high probability i.e.: \(\mathbb{P}\bigg[\frac{\sum_i^n X_i}{n} - \mathbb{E}\bigg(\frac{\sum_i^n X_i}{n}\bigg) \geq \epsilon \bigg] \textrm{ gets smaller and smaller with n}\)</p> <h3 id="markovs-inequality">Markov’s Inequality</h3> <p>Take a non-negative scalar values RV $X$. (i.e. $X \geq 0$ always) Then for every positive cutoff $a$, we have the following: \(\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a}\) The right tail probability is smaller than the expectation value divided by the tail begining</p> <p>Another form of this: \(\mu = \mathbb{E}(X) \\ \mathbb{P}(X \geq k\mu) \leq \frac{1}{k}\)</p> <blockquote> <p><strong>The probability of being atleast within $k$ means of $0$ is at most $1/k$</strong></p> </blockquote> <h4 id="proof-of-markovs-inequality">Proof of Markov’s Inequality</h4> \[\begin{align} \mathbb{E}(X) &amp;= \int_0^a p(x) x dx + \int_a^\infty p(x)x dx \\ \textrm{ second integrand's x is bigger than $a$} \\ \mathbb{E}(X) &amp;\geq \int_0^a p(x) x dx + \int_a^\infty p(x)a dx \\ \textrm{ first integrand has to be bigger than $0$} \\ \mathbb{E}(X) &amp;\geq a\int_a^\infty p(x) dx \\ \mathbb{E}(X) &amp;\geq a\mathbb{P}(X \geq a) \\ \mathbb{P}(X \geq a) \leq \frac{\mathbb{E}(X)}{a} \end{align}\] <h3 id="inequality-is-weak">Inequality is weak</h3> <p>You can try this out for the dice and find that’s a pretty weak constraint.</p> <h2 id="lecture-2">Lecture 2</h2> <p>If we have bounds on “higher moments” of $X$, then we can do better than Markov’s inequality.</p> <p>For every positive even integer $r$, even if $X$ takes negative values.</p> <p>Then \(P(X \geq a) \leq P(X^r \leq a^r) \textrm{ this is an equality if } X \geq 0\) This implies that: \(P(X \geq a) \leq \frac{\mathbb{E}(X^r)}{a^r}\) Only if $\mathbb{E}(X^r)$ is finite (if its infinite then it doesn’t really say an)</p> <p>The first application of this is chebyshev’s inequality.</p> <h3 id="chebyshevs-inequality">Chebyshev’s Inequality</h3> \[P(|X - \mathbb{E}(X)| \geq c) \leq \frac{Var(X)}{c^2}\] <p>Equivalently $\mathbb{E}(X) = \mu$, $Var(X) = \sigma^2$ and $c = k\sigma$ then the inequality reads: \(P(\|X-\mu\| \geq k\sigma) \leq \frac{1}{k^2}\)</p> <blockquote> <p><strong>Any RV is more than $k$ standard deviations away from its mean $1/k^2$ of the time</strong></p> <p>e.g: Any variable (with finite variance) is 2 std away from it’s mean less than 1/4th of the time</p> </blockquote> <h5 id="proof">Proof:</h5> <p>Use Markov’s inequality \(P((X-\mathbb{E}(X))^2 \geq c^2) \leq \frac{(X-\mathbb{E}(X))^2}{c^2} \\ P(\|X - \mathbb{E}(X)\| \geq c) \leq P((X-\mathbb{E}(X))^2 \geq c^2) \leq \frac{Var(X)}{c^2} \\\)</p> <h3 id="master-tail-bound">Master Tail Bound</h3> <p>Let $Y = X_1 + X_2 + … + X_n$ where the $X_i$ are mutually independent <strong>but not necessarilly identically distributed</strong> with conditions:</p> <ol> <li>Centered: $\mathbb{E}(X_i) = 0$</li> <li>Variance Bound: $\mathbb{E}(X^2_i) \leq \sigma^2$</li> <li>Higher moment bound: $|\mathbb{E}(X^s_i)| \leq s!\sigma^2$ for upto $s=3,4,…,\lfloor\frac{a^2}{4n\sigma^2} \rfloor$</li> </ol> <p>If $a$ is <strong>not too big:</strong> $0 &lt; a \leq \sqrt{2}n \sigma^2$ Then this variable’s upper bound <strong>decays like a gaussian</strong>: \(P(\|Y\| \geq a) \leq 3e^{-\frac{a^2}{12n\sigma^2}}\)</p> <h5 id="comments">Comments</h5> <p>Can rewrite the conclusion in another way ($a = \sqrt{n} t$): \(P(\frac{\|Y\|}{\sqrt{n}} \geq t) \leq 3 e^{-\frac{t^2}{12\sigma^2}}\) Can also rewrite using a sample average $a = nb$: \(P(\frac{\|Y\|}{n} \geq b) \leq 3 e^{-\frac{nb^2}{12\sigma^2}}\) Comparison to the <strong>Central Limit Theorum (CLT)</strong>:</p> <p>Let $X_1, X_2, …X_n$ be iid with mean 0 and variance $\sigma^2$ each. \(\lim_{n \to \infty}\frac{Y}{\sqrt{n}} \sim \mathcal{N}(0,\sigma^2)\)</p> \[P(\frac{\|Y\|}{\sqrt{n}} \geq t) \leq e^{-\frac{t^2}{2\sigma^2}} \textrm{ by CLT} \\\] \[P(\frac{\|Y\|}{\sqrt{n}} \geq t) \leq 3e^{-\frac{t^2}{12\sigma^2}} \textrm{ by Master Tail Bound} \\\] <p>Takeaway: <strong>Master Tail bound is a non-asymptotic, slightly weaker CLT-type bound</strong></p> <h3 id="proof-of-master-tail-bound">Proof of Master Tail Bound</h3> \[P(\|Y\| \geq a) = P(Y^r \geq a^r) \textrm{ for any positive integer r} \\ P(\|Y\| \geq a) \leq \frac{\mathbb{E}(X^r)}{a^r}\] <p>Use the multinomial expansion: \(X^r = (X_1 + X_2 + .. X_n)^r = \sum_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}X_1^{r_1} X_2^{r_2} ... X_n^{r_n}\)</p> \[\mathbb{E}(X^r) = \mathbb{E}((X_1 + X_2 + .. X_n)^r) = \sum_{r_1+r_2 + ...r_n = r} \frac{r!}{r_1!r_2!...r_n!}\mathbb{E}(X_1^{r_1} X_2^{r_2} ... X_n^{r_n})\] <p>Use mutual independence: \(\mathbb{E}(X^r) = \sum_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}\mathbb{E}(X_1^{r_1}) \mathbb{E}(X_2^{r_2}) ... \mathbb{E}(X_n^{r_n})\)</p> <blockquote> <p>Note that if there is any term in the expansion with $r_i = 1$, then by $\mathbb{E}(X_i) = 0$, that whole term is $0$</p> </blockquote> \[\mathbb{E}(X^r) = \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}\mathbb{E}(X_1^{r_1}) \mathbb{E}(X_2^{r_2}) ... \mathbb{E}(X_n^{r_n})\] <p>Use the triangle inequality: \(\mathbb{E}(X^r) \leq |\mathbb{E}(X^r)| \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}|\mathbb{E}(X_1^{r_1}) \mathbb{E}(X_2^{r_2}) ... \mathbb{E}(X_n^{r_n})|\)</p> \[\mathbb{E}(X^r) \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!}|\mathbb{E}(X_1^{r_1})|| \mathbb{E}(X_2^{r_2}) |... |\mathbb{E}(X_n^{r_n})|\] \[\begin{align} \mathbb{E}(X^r) \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r}\frac{r!}{r_1!r_2!...r_n!} (\sigma^2)^{\textrm{\# of non zero } r_i \textrm{s}} r_1!r_2!..r_n! \\ \mathbb{E}(X^r) \leq \sum^{r_i \geq 2}_{r_1+r_2 + ...r_n = r} (\sigma^2)^{\textrm{\# of non zero } \textrm{s}} r!\\ \textrm{ SOME COMBINATORIAL TRICKERY LOOK AT THE BOOK} \\ \mathbb{E}(X^r) \leq \frac{r!}{(r/2)!}2^{r/2}(n\sigma^2)^{r/2} \end{align}\] <p>Hence we have: \(P(\|Y\|\geq a) \leq \frac{\mathbb{E}(X^r)}{a^r} \leq \frac{r!}{a^r(r/2)!}2^{r/2}(n\sigma^2)^{r/2} = g(r)\) Let’s find the $r$ that minimizes the right hand side, and gives us the best bound: \(\frac{r!}{a^r(r/2)!} \textrm{ is at most } r^{r/2}\) and hence: \(g(r) \leq \frac{r!}{a^r(r/2)!}2^{r/2}(n\sigma^2)^{r/2} \leq \bigg(\frac{2rn\sigma^2}{a^2}\bigg)^{r/2}\) We then choose $r$ to be the largest even number s.t $r \leq \frac{a^2}{6n\sigma^2}$</p> <p>This makes it such that: \(g(r) \leq (\frac{1}{3})^{r/2}\) this means that for the lols we can do $3 \to e$: \(g(r) \leq (\frac{1}{e})^{r/2} = e^{-r/2} \leq e^{-\frac{a^2}{12n\sigma^2} + \frac{2}{2}}\) Again because we choose the smallest even number do: \(g(r) \leq e^{-\frac{a^2}{12n\sigma^2} + 1} \leq 3 e^{-\frac{a^2}{12n\sigma^2}}\) This gives us our bound: \(P(\|Y\|\geq a) \leq 3 e^{-\frac{a^2}{12n\sigma^2}}\)</p> <h3 id="applying-the-tail-bound">Applying the tail bound</h3> <p>Assume $X_i \sim Bernoulli(p)$, then what is the tail bound on $Y = \sum^n_i X_i$, where now $Y \sim Binomial(n,p)$ ?</p> <p>For any $c \in [0,1]$ whe have: \(P(\|\frac{Y}{n} - p\| \geq \sqrt2 c p (1-p)) \leq 3 e^{-\frac{np(1-p)c^2}{6}}\) Proof:</p> <p>Let $Y_i = X_i - p$ then $\mathbb{E}(Y_i) = 0$ and $Var(Y_i) = Var(X_i) = \mathbb{E}(Y_i^2)= p(1-p) $ \(\mathbb{E}(Y_i^s) = \|p(1-p)^s + (1-p)(-p)^s\| \leq p(1-p)^s + (1-p)(-p)^s \\ \mathbb{E}(Y_i^s) \leq p(1-p)^2 + (1-p)(p)^2 \\ \mathbb{E}(Y_i^s) \leq \frac{p(1-p)}{3} \\\) Plug in $\sigma^2 = p(1-p)$ into MTB and get the bound as given before.</p> <h1 id="lecture-3">Lecture 3</h1> <p>Chernov bound discussion:</p> <p>Assume $X_i \sim Bernoulli(p)$, then what is the tail bound on $Y = \sum^n_i X_i$, where now $Y \sim Binomial(n,p)$ ?</p> <p>For any $c \in [0,1]$ whe have: \(P(\|\frac{Y}{n} - p\| \geq \sqrt2 c p (1-p)) \leq 3 e^{-\frac{np(1-p)c^2}{6}}\)</p> <blockquote> <p><strong>remark</strong>: How sharp is this estimate?</p> <p><strong>Ans</strong>: Not so tight. One can fix $n,p $ and $c$, and just estimate this probabiliity.</p> </blockquote> <p>The constraint is not too tight. See below:</p> <p><img src="2023-03-25-FODS-lecture-1.assets/chernovbound.svg" alt="chernovbound"/></p> <p>Other ineqs: Bernstein’s inequality and Hoeffding’s inequality. But these follow approximately the same methods we used i.e. Applying markov to a modification of a Random RV.</p> <h2 id="curses-and-blessings-in-high-dimensions">Curses and Blessings in High dimensions</h2> <h2 id="curses">Curses:</h2> <p>$\mathbb{R}^d$ where $d$ is large, e.g. $10^6$</p> <blockquote> <p><strong>Function approximation:</strong></p> <p>Given points $(x_1,y_1),(x_2,y_2),…, (x_n,y_n)$ in $\mathbb{R}^d \times\mathbb{R}^d$, find a good $f: \mathbb{R}^d \to \mathbb{R}^d$ such that $f(x_i) = y_i$ or $f(x_i) \approx y_i$</p> <p>Approach: Linear Interpolation, polynomial interpolation, spline interpolation. These are all piecewise interpolations, which can be a problem for large $d$</p> <p><strong>Question:</strong> How many d-dim cubes with side length $\epsilon$ are needed to fill a cube with side length 1</p> <p><strong>Answer</strong>: $(\frac{1}{\epsilon})^d$</p> </blockquote> <p>We need a way to overcome large $d$, i.e. $d$ is exponentional. Because piecewise clearly will not be doable.</p> <p>We overcome this by putting assumptions on $f$. Examples are:</p> <ul> <li>$f$ is affine linear and the matrix that represents that linear function is low-rank</li> <li>$f$ is …</li> </ul> <h2 id="blessings">Blessings:</h2> <p>Various natural probability distributions on $\mathbb{R}^d$ will concentrate more and more as $d$ increases.</p> <p>But first we need to talk about spheres and balls in $\mathbb{R}^d$. Main theorum is that a Gaussian annulus theorum.</p> <h4 id="high-dimensional-ball">High Dimensional Ball</h4> <p>Definition: A $d$ dimensional ball $B^d$ is given \(B^d = \{x \in \mathbb{R}^d : x_1^2 + x_2^2 + ... + x_d^2 \leq 1\}\)</p> <h4 id="rejection-sampling">Rejection Sampling</h4> <p>One way is rejection sampling which is BAD for high dimensions. \(vol(B^d) = \frac{2\pi}{\Gamma(\frac{d}{2})d}\) <strong>Corollary 1</strong>: \(vol(B^d) \to 0 \textrm{ as } d \to 0\) <strong>Corollary 2</strong>:</p> <p>The acceptance probability of the rejection sampling scheme goes to $0$ as $d \to \infty$</p> <h4 id="solution">Solution</h4> <p>Sample from a spherical gaussian in $\mathbb{R}^d$.</p> \[X \sim \mathcal{N}(0,\mathbb{I}) \\ p = \prod_i \phi(x_i)dx_i\] <p>Note that this distribution is rotationally symmetric, since it depends on a rotationally invariant scalar $X^T X$.</p> <p>Note then that:</p> \[Y = \frac{X}{\|X\|_2} \textrm{ is uniform on the sphere}\] <p>Then let there be a positive scalar whose density function is:</p> \[p(\rho) \propto \rho^{d-1}\] <p>This must be because in $n$ dimensional polar coordinates:</p> \[dx_1\wedge dx_2 \wedge ... \wedge dx_n = \rho^{d-1}d\rho \wedge d^{d-1}\Omega\] <p>So we have the fact that you can get the uniform distribution by:</p> <ul> <li>Define $Y = \frac{X}{|X|_2}$</li> <li>Then get a random variable $\rho \in [0,1]$ with density $\rho^{d-1}$</li> <li>$U \sim \frac{\rho X}{|X|_2}$ is uniform in the unit ball!</li> </ul> <h3 id="gaussian-annulus-theorem">Gaussian Annulus Theorem</h3> <p>Let $X \sim \mathcal{N}(0,I)$ For any $\beta \in [0,\sqrt{d}]$: \(P(|\ \|X\|_2 - \sqrt{d}\ | \geq \beta) \leq 3e^{-\frac{\beta^2}{96}}\) We can prove this using the master tail bound. Let there be $d$ independent random variables $x_i^2 - 1$ which are distributed as $\chi^2$ distributions. Then we define \(Y = \sum^d_i (x_i^2-1)\) Hence the bound above just becomes: \(P(|\ \|X\|_2 - \sqrt{d}\ | \geq \beta) = P(|\ \|X\|_2^2 - d\ | \geq \beta^2) = P(\|Y\| \geq \beta^2)\) Now we must have that: \(\mathbb{E}(x^2_i - 1) = 0 \\ \mathbb{E}((x^2_i - 1)^2) \leq \sigma^2 \\ \|\mathbb{E}((x^2_i - 1)^s)\| \leq \sigma^2 s! \\ \implies P(\|Y\| \geq \beta^2) \leq 3e^{-\frac{\beta^4}{12d\sigma^2}}\) To show the above we can do the following bounding process: \(\|\mathbb{E}((x^2_i - 1)^s)\| \leq \mathbb{E}(\|(x^2_i - 1)^s\|) \leq \mathbb{E}(1 + x^{2s}_i) = 1 + \mathbb{E}(x^{2s}_i) \leq 2^s s!\) Apply master tail bound to $\frac{Y_i}{2}$</p>]]></content><author><name>Asad Hussain</name></author><category term="statistics"/><category term="DataScience"/><category term="MachineLearning"/><summary type="html"><![CDATA[Some notes from the first lecture of the course Foundations of Data Science at the Odin Institute at UT Austin. It's great to go into the basics and derive all these fundamental theorems]]></summary></entry></feed>